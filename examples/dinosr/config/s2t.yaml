# @package _group_

common:
  fp16: true
  log_format: json
  log_interval: 20
  tensorboard_logdir: tb
  profile: false

checkpoint:
  save_interval: 5
  save_interval_updates: 5000
  keep_interval_updates: 1
  no_epoch_checkpoints: true
  load_checkpoint_on_all_dp_ranks: true

task:
  _name: speech_to_text
  data: ???
  max_sample_size: 480_000 # 30 seconds 
  min_sample_size: 16_000 # 1 second
  normalize: true

dataset:
  num_workers: 6
  #max_tokens: 3800000
  batch_size: 2
  database_file: "/workspace/fairseq/data/small_database.csv"
  vocab_file: "/workspace/fairseq/data/vocab.csv" 
  skip_invalid_size_inputs_valid_test: true
  validate_interval: 5
  required_batch_size_multiple: 1
  disable_validation: true

distributed_training:
  distributed_world_size: 1
  ddp_backend: legacy_ddp

criterion:
  name: CTCLoss
  params:
    blank: 481

optimization:
  update_freq:
    - 16
  max_update: 400000
  lr: [0.0005]

optimizer:
  name: Adam
  params:
    lr: 0.0005
    betas: [0.9,0.98]
    eps: !!float 1e-06
    weight_decay: 0.01

lr_scheduler:
  _name: tri_stage
  phase_ratio: [0.03,0.47,0.50]

model:
  extractor_mode: layer_norm
  encoder_embed_dim: 768
  fe_dropout: 0.0

  conv_feature_enc: "[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]"
  vocab_file: "/workspace/fairseq/data/vocab.csv"
